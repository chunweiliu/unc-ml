\documentclass{article}
\include{hwdefs}
\setcounter{HW}{1}
\begin{document}

\author{Chun-Wei Liu}
\title{COMP  790-124, HW\theHW}
\maketitle



\newproblem{0.01pt} Open \texttt{hw\theHW.tex}, replace ``Wile E. Coyote'' with your name. Run
\texttt{pdflatex hw\theHW.tex}, look at hw\theHW.pdf, and confirm that your name is in the right place.


\newproblem{1pt}
\begin{enumerate}
\item Plot the sigmoid function in MATLAB using script
\begin{verbatim}
z = [-5:0.1:5];
fz = 1./(1 + exp(-z));
plot(z,fz,'LineWidth',3);
xlabel('z');ylabel('f(z)'); % we always label axes, yes we do!
hwplotprep
print -dpdf sigmoid.pdf
\end{verbatim}
Find the resulting figure in file {\tt sigmoid.pdf}.
\item In hw\theHW.tex, find the segment of the file that sets up the first figure -- it starts with {\tt \textbackslash begin\{figure\}} and ends with  {\tt \textbackslash end\{figure\}}. Inside this segment  replace {\tt emptiness.pdf} with {\tt sigmoid.pdf}.
\item Change the text under {\tt \textbackslash caption} -- right now it says ``This is emptiness, it earns no points.'' -- to say what the figure is about.
\item Remake hw\theHW.pdf by running in shell/command prompt

     \texttt{pdflatex hw\theHW.tex}

and check that your plot and caption are now in.
\end{enumerate}


\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{sigmoid.pdf}
\caption{This is a sigmoid function.}
\end{center}
\end{figure}

\hrule

\newproblem{1pt}
Fill in the first derivative and second derivative of sigmoid function in the hw\theHW.tex.

The first derivative
\[
\frac{d f(z)}{dz} =  \frac{\exp(-z)}{(1+\exp(-z))^2}.%here goes your derivative, but before the % character otherwise it will be commented out
\]

The second derivative
\[
\frac{d^2 f(z)}{d^2z} = \frac{2\exp(-2z)}{(1+\exp(-z))^3} 
- \frac{\exp(-z)}{(1+\exp(-z))^2}
\]

You might have to consult an intro to \LaTeX in order to figure out how to format your math.

\newproblem{1pt}
Write a MATLAB function that implements computation  of the first derivative of $f$ at a particular point. You just did the math for this.
Here is a function that is {\em right}
\begin{verbatim}
function d = dsigmoid(z)
%DSIGMOID computes first derivative of sigmoid function at z
d = exp(-z)/(1+exp(-z))^2;
end
\end{verbatim}
Correct hw\theHW.tex by replacing {\tt ...} above with the correct MATLAB code to compute expression you obtained in previous problem.

Crate a file {\tt dsigmoid.m} that {\em correctly} computes the first derivative.

\newproblem{1pt}

We will use your function {\tt dsigmoid.m} to plot the first derivative.
\begin{verbatim}
zs = [-5:0.01:5];
for i = 1:length(zs)
    ds(i) = dsigmoid(zs(i));
end
plot(zs,ds,'LineWidth',3);
xlabel('z');ylabel('df(z)');
hwplotprep
print -dpdf dsigmoid.pdf
\end{verbatim}

Find the resulting plot in file {\tt dsigmoid.pdf}. In hw\theHW.tex replace {\tt emptiness.pdf} with {\tt dsigmoid.pdf} . Change the
caption in the figure to say what the figure is about. Remake hw\theHW.pdf and check that your plot has made it in.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{dsigmoid.pdf}
\caption{This is the first derivative of the sigmoid function.}
\end{center}
\end{figure}


\newproblem{1pt}
We can approximate derivatives numerically
\[
\frac{df(z)}{dz}\approx \frac{f(z+h) - f(z)}{h}
\]
where the right-side of this approximate equality is called {\em finite difference} approximation. Unlike derivative definition we do not need $h$ to be infinitesimal, just a small value. The numerical approximation of a derivative is tremendously useful trick to check you derivative, gradients, Jacobians, Hessians etc. Make sure that you understand what it does.

We will use this approximation to check your derivatives. Here is a function that computes approximately the derivatives of sigmoid
\begin{verbatim}
function d = fdsigmoid(z)
f0 = 1/(1 + exp(-z));
f1 = 1/(1 + exp(-(z + 1e-5)));
d = (f1 - f0)/1e-5;
end
\end{verbatim}
Save this function into a file names \texttt{fdsigmoid.m}.

Try following code in MATLAB
\begin{verbatim}
zs = randn(100,1);
for i=1:length(zs)
    err(i) = dsigmoid(zs(i)) - fdsigmoid(zs(i));
end
hist(err,30)
hwplotprep
print -dpdf hist.pdf
\end{verbatim}
The code above samples 100 normally distributed values and computes the finite differences approximation and the derivative you derived and implemented and then plots histogram of errors.

Find the resulting plot in file {\tt hist.pdf}. In hw\theHW.tex replace {\tt emptiness.pdf} with {\tt hist.pdf} . Change the
caption in the figure to say what the figure is about. Remake hw\theHW.pdf and check that your plot has made it in.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{hist.pdf}
\caption{Histogram of errors between finite element method and analytic solution of $df(z)$.}
\end{center}
\end{figure}



\begin{remark} The error ranges between $-5\times10^{-7}$ and $5\times10^{-7}$.
\end{remark}

\newproblem{1pt}
From Taylor's theorem (first year calculus) we can obtain
\[
f(z+h) = f(z) + \frac{df(z)}{dz}h + \frac{1}{2}\frac{d^2f(z)}{d^2z}h^2 + O(h^3).
\]
Derive a bound on the error of the finite differences approximation using the above expression. You can use big O notation to express this bound.
\[
\textrm{Err}(z_0,h) = \abs{\frac{f(z_0+h) - f(z_0)}{h} - \frac{df(z_0)}{dz}} \leq \frac{1}{2}\frac{d^2f(z_0)}{d^2z}h+O(h^2)
\]

Specifically for sigmoid function plug in appropriate derivative on the right hand side of the inequality. If $h=10^{-5}$ and $z_0 = 0$ the error of the finite difference should be about $10^{-10}$.
Does this agree with the histogram of error that is in the figure above?
No, this does not agree with the histogram. 
Yet the $z$ was generated from a normal distribution with zero mean and standard deviation one, so the $z$ is not fit the $z_0$ setting here.
\newproblem{1pt}
Let
\BEQ\label{eq:pz}
f(z) = \frac{1}{1 + \myexp{-z}} = p
\EEQ
express $z$ in terms of $p$
\[
z = \log\frac{p}{1-p}.
\]
Now suppose
\BEQ\label{eq:qz}
\frac{\myexp{-z}}{1 + \myexp{-z}} = q
\EEQ
and express $z$ in terms of $q$
\[
z = \log\frac{1-q}{q}.
\]
Given Eqs.\eqref{eq:pz},\eqref{eq:qz} express $q$ in terms of $p$
\[
q = 1-p.
\]
Express $f(-z)$ in terms of $f(z)$
\[
f(-z) = \frac{1}{1+\exp(z)} = \frac{1}{1+\frac{1}{\exp(-z)}} = 1-f(z).
\]
Hint: the manipulations that are useful here are either subtraction from 1 (as in $1-x$), computing inverse (as in $\frac{1}{x}$), and taking logarithm (as in $\log(x)$).

\section*{Log of sigmoid}
\newproblem{1pt}
Let $g(z)$ be log of sigmoid function
\[
g(z) = \mylog{ \frac{1}{1+ \myexp{-z}} }.
\]
Compute its derivative and fill it in here
\[
\frac{dg(z)}{dz} =  \frac{\exp(-z)}{1+\exp(-z)}.%here goes your derivative of log sigmoid, but before the % character otherwise it will be commented out
\]
Check your derivative by comparing its value to the finite difference approximation.


\newproblem{1pt}
Compute second derivative of $g(z)$
\[
\frac{d^2g(z)}{d^2z} =  \frac{\exp(-2z)}{(1 + \exp(-z))^2} - \frac{\exp(-z)}{1 + \exp(-z)}.%here goes your second derivative of log sigmoid, but before the % character otherwise it will be commented out
\]
Check the second derivative by comparing its value to the finite difference of the {\em first} derivatives you computed above.


\newproblem{1pt}
Let the dataset be specified by $\mathcal{D} = \left\{ (\xx_i,y_i):i=1,\dots,n \right\}$. We specify conditional probability of $y$
\BEQ \label{eq:plr}
p(y_i | \xx_i,\beta_0,\beta) = \frac{1}{1 + \myexp{-y_i(\beta_0 + \bket{\beta}{\xx_i})}}
\EEQ
Write a matlab function that computes log probability of label $y$ given a vector of features $\xx$ and $\beta_0,\beta$.
\begin{verbatim}
function logP = logProbLogReg(y, X, beta0, beta)
logP = log(1/(1+exp(-y*(beta0+beta*X))));
\end{verbatim}
Now write a matlab function that uses the above function to compute log probability of label $+1$ for a vector of features $\xx$ and $\beta_0,\beta$
\begin{verbatim}
function predY = predictY(X, beta0, beta)
logProbY = logProbLogReg(1,X,beta0,beta);
if logProbY > log(0.5)
    predY = 1;
else
    predY = -1;
end
\end{verbatim}


\newproblem{1pt}
Given Eq.\eqref{eq:plr} we can write out log-likelihood
\BEQ \label{eq:ll}
\textrm{LL}(\beta_0,\beta;\mathcal{D}) = \sum_i \log \frac{1}{1 + \myexp{-y_i(\beta_0 + \bket{\beta}{\xx_i})}}.
\EEQ
Now using function $\texttt{logProbLogReg}$ that you obtained for the previous problem, write a matlab function that computes loglikelihood
\begin{verbatim}
function val = LogLikLogReg(y, X, beta0, beta)
val = 0;
for i=1:length(y)
   val = val + logProbLogReg(y(i),X(:,i),beta0,beta);
end
\end{verbatim}
\newproblem{1pt}
Write a function that computes gradient of log-likelihood of logistic regression Eq.\eqref{eq:ll}
\begin{verbatim}
function [dbeta0, dbeta] = dLogLikLogReg(y, X, beta0, beta)
p = sigmoid(y.*(beta0+beta*X));
dbeta0 = sum((1-p).*y);
dbeta = zeros(1, length(beta));
for i = 1:length(beta)
    dbeta(i) = sum((1-p).*y.*X(i,:));
end
\end{verbatim}
You can make sure that your implementation is correct using the finite differences trick.
\newproblem{1pt}
Implement a gradient ascent algorithm for fitting logistic regression and paste it below.
\begin{verbatim}
function [beta0,beta,vals] = fitLogReg(trainY,trainX)
% random initialize beta0 and beta, and use gradient decent for updating
s = 1e-5;
max_iterations = 2000;

beta0 = randn(1,1);
beta = randn(1,size(trainX,1));

vals = zeros(1,max_iterations);
for i = 1:max_iterations
    % implement gradient assent
    [dbeta0, dbeta] = dLogLikLogReg(trainY,trainX,beta0,beta);
    beta0 = beta0 + s * dbeta0;
    beta = beta + s * dbeta;
    
    vals(i) = LogLikLogReg(trainY,trainX,beta0,beta);
end
\end{verbatim}
Run it with fixed step size $s=10^{-5}$, for 2000 iterations, on data stored in \texttt{hw\theHW.mat}.
Note that \texttt{load hw\theHW.mat} loads the $y$ and $X$ variables, on which you can run by issuing command
\texttt{[beta0,beta] = fitLogReg(trainY,trainX)}.
Report resulting $\beta_0,\beta$
\begin{verbatim}
beta0 = 0.4014
beta1 = -0.0814
    0.4614
   -0.7830
    0.0794
    0.5872
    0.3059
    0.6428
   -1.2271
    0.3921
   -0.2538
   -0.6873
    0.5192
   -1.0525
   -0.2977
    0.1188
   -0.8931
   -0.1050
    0.9571
    2.4523
   -0.2437
    0.4286
    0.9721
   -0.9611
    0.3502
   -0.4589
   -1.0973
   -1.7244
    0.9582
    0.2371
   -0.4038
    1.9145
    1.0881
    0.2125
    1.0028
   -1.6309
   -1.6809
    1.5527
    1.1132
   -0.3175
    0.6127
   -0.4944
    1.7440
    0.3410
    0.0133
    2.7733
   -0.2986
    0.4090
    1.1396
    0.4683
    0.3427
    0.5558
    0.2503
    0.1514
    0.1994
   -0.6523
    1.6098
   -1.4253
    0.1139
    0.8355
   -1.2687
   -0.7048
   -0.3888
    1.8800
    0.7712
    0.8059
    0.5871
   -0.7488
   -0.2027
    0.1669
   -0.1812
   -0.2907
    1.2908
   -2.0059
    0.6116
   -0.2946
    0.4223
    1.0970
   -0.3876
   -0.5805
    0.3333
    0.6448
    0.8396
   -0.2588
    0.4066
   -1.3037
   -0.1332
    0.7545
   -0.4209
    0.5070
   -0.8282
    1.1472
   -0.9806
    2.5761
    0.1621
    0.1255
    0.8708
   -0.3208
    0.1725
    0.4539
   -0.2096
    0.3865
   -0.5265
   -0.4557
    0.0700
   -0.2294
    0.4627
   -0.5894
   -0.2656
   -2.1903
    0.3652
    0.9997
    0.2185
   -0.0492
   -0.4547
   -0.5484
    1.4909
   -0.7096
    1.0925
    0.5061
    0.9411
   -0.1304
   -1.9101
    0.6649
    0.6308
   -1.5803
   -2.2752
    1.3538
   -0.9052
   -0.2814
    0.4159
   -0.1370
    1.9102
   -0.6988
    1.2273
   -0.0027
    0.0705
   -0.2439
   -0.3423
   -1.1063
    0.4762
    1.7866
    2.1204
   -0.5587
    0.7040
   -0.2291
   -1.0324
    0.2113
   -0.3984
    0.6754
    0.7710
    1.7097
    0.6808
    1.2750
   -0.5570
   -1.5139
    0.6237
    0.5195
   -0.3622
    1.0920
   -0.4615
    2.1373
    0.7866
   -1.6086
   -1.6719
    0.0910
   -0.7058
   -0.9504
   -0.1875
   -0.3700
   -1.3068
   -0.2660
   -1.5738
    0.6606
   -1.4942
   -0.4660
    0.1477
    0.6513
    1.4815
    0.8487
    1.6084
   -1.7080
   -0.1781
    0.1900
   -0.7782
   -0.5301
    1.3319
   -0.3674
   -0.4165
   -1.7484
   -0.1567
   -0.0084
   -1.5694
    0.5366
   -0.5326
    1.2501
   -2.2415
   -0.7816
    0.8263
    1.2420
   -1.3111
   -1.0491
   -0.5008
    0.0184
    0.7654
   -0.9235
    0.0812
    0.2519
    1.1819
   -1.3769
    0.0730
    0.1825
   -0.3287
   -1.9147
    0.9492
    1.1265
   -0.3402
   -0.5399
   -0.7939
    0.5300
   -0.9704
    1.3760
   -0.3353
    0.9993
   -0.0841
    1.0034
   -0.9024
    1.7623
   -1.2073
    2.3860
   -0.3957
    0.2343
    0.1150
    1.5160
    0.2963
   -0.6027
   -0.0637
\end{verbatim}
\newproblem{1pt}
Implement estimation of prediction error using cross validation
\begin{verbatim}
load hw1.mat
rand(’seed’,1); K = 5; N = length(y);
indices = crossvalind(’Kfold’,N,K);
for k=1:K
    testX = x(:,indices == k);
    testY = y(indices == k);
    trainX = x(:,indices ~= k);
    trainY = y(indices ~= k);
    [beta0,beta] = fitLogLikLogReg(trainY,trainX);
    for i=1:length(testY)
        predY = predictY(X,beta0,beta)
        err(k) = err(k) + (testY ~= predY);
    end
end
cvErr = sum(err)/length(y);
\end{verbatim}
Once done, run this on data stored in \texttt{hw\theHW.mat}. The cross-validation estimate of error on that dataset is 0.2718 after 2,000 iterations. Figure~\ref{fig:log2000} showed the log likelihood. The result were improved to 0.1724 after 4,000 iterations, to 0.0805 after 6,000 iterations, to 0.0345 after 8,000 iterations, and 0.0115 after 10,000 iterations. The betas are converged then. Figure~\ref{fig:beta12000} illustrated the final beta.

\begin{figure}[T]
\begin{center}
\includegraphics[height=50mm]{loglikeli2000.png}
\caption{The log likelihood after updates in 2,000 iterations.}
\label{fig:log2000}
\end{center}
\end{figure}

\begin{figure}[T]
\begin{center}
\includegraphics[height=50mm]{beta12000.png}
\caption{The beta after updates in 12,000 iterations.}
\label{fig:beta12000}
\end{center}
\end{figure}

\end{document}
